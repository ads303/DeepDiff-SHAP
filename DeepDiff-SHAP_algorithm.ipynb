{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcc36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from scipy.stats import f, levene\n",
    "import shap\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79209f-6f54-4737-9a72-29948b6e6da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fetch dataset\n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
    "\n",
    "# Features and target\n",
    "X = cdc_diabetes_health_indicators.data.features\n",
    "y = cdc_diabetes_health_indicators.data.targets\n",
    "\n",
    "# X is a pandas DataFrame\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# # metadata \n",
    "# print(cdc_diabetes_health_indicators.metadata) \n",
    "  \n",
    "# # variable information \n",
    "# print(cdc_diabetes_health_indicators.variables) \n",
    "\n",
    "# Combine for easier slicing\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Ensure target column is called 'Diabetes_binary'\n",
    "target_col = 'Diabetes_binary'\n",
    "\n",
    "# Subset by class label\n",
    "class_0 = data[data[target_col] == 0]\n",
    "class_1 = data[data[target_col] == 1]\n",
    "\n",
    "# Drop target column to leave only features\n",
    "X0_df = class_0.drop(columns=[target_col])\n",
    "X1_df = class_1.drop(columns=[target_col])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X1 = X0_df.to_numpy()  # Class 0 → call this X1 for consistency with your example\n",
    "X2 = X1_df.to_numpy()  # Class 1 → call this X2\n",
    "\n",
    "# Confirm shape and type\n",
    "print(f\"X1 shape (Diabetes = 0): {X1.shape}\")\n",
    "print(f\"X2 shape (Diabetes = 1): {X2.shape}\")\n",
    "\n",
    "# Data preprocessing\n",
    "# Prior to running DeepDiff-SHAP DCI, the data should be at least mean cenetered (along with any other transformations).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X2 = scaler.fit_transform(X2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac46ca-d8a4-4f72-b836-fd6ebc7af241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating example data with feature dependence (F0-F3, F0-F7, F9-F14, F9-F17)\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# np.random.seed(42)\n",
    "\n",
    "# n = 200\n",
    "# p = 20\n",
    "\n",
    "# # Start with Gaussian noise\n",
    "# X1 = np.random.randn(n, p)\n",
    "# X2 = np.random.randn(n, p)\n",
    "\n",
    "# # Inject strong dependence in opposite directions\n",
    "# # In X1, X0 depends heavily on X3\n",
    "# X1[:, 0] = 11 * X1[:, 3] + np.random.normal(0, 0.1, size=n)\n",
    "\n",
    "# # In X2, X0 depends heavily on X7 instead\n",
    "# X2[:, 0] = 5 * X2[:, 7] + np.random.normal(0, 0.1, size=n)\n",
    "\n",
    "# # Inject strong dependence in opposite directions\n",
    "# # In X1, X0 depends heavily on X3\n",
    "# X1[:, 9] = 9 * X1[:, 14] + np.random.normal(0, 0.1, size=n)\n",
    "\n",
    "# # In X2, X0 depends heavily on X7 instead\n",
    "# X2[:, 9] = 8 * X2[:, 17] + np.random.normal(0, 0.1, size=n)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X1 = scaler.fit_transform(X1)\n",
    "# X2 = scaler.fit_transform(X2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from numpy.linalg import pinv\n",
    "from scipy.special import ncfdtr\n",
    "import itertools as itr\n",
    "\n",
    "def deepdiff_shap_undirected_graph(X1, X2, difference_ug_method='constraint', alpha=0.05, verbose=0):\n",
    "    \"\"\"\n",
    "    Estimates the difference between two undirected graphs directly from two data sets\n",
    "    using constraint-based method that relies on comparing precision matrices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1: array, shape = [n_samples, n_features]\n",
    "        First dataset.    \n",
    "    X2: array, shape = [n_samples, n_features]\n",
    "        Second dataset.\n",
    "    difference_ug_method: str, default = 'constraint'\n",
    "        Method for computing the undirected difference graph. Only 'constraint' supported.\n",
    "    alpha: float, default = 0.05\n",
    "        Significance level parameter for hypothesis testing.\n",
    "        Higher alpha leads to more edges in the difference undirected graph.\n",
    "    verbose: int, default = 0\n",
    "        The verbosity level of logging messages.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    difference_ug: set of frozensets\n",
    "        Set of frozenset edges in the difference undirected graph.\n",
    "    nodes_cond_set: set\n",
    "        Nodes to be considered as conditioning sets.\n",
    "    \"\"\"\n",
    "    if difference_ug_method != 'constraint':\n",
    "        raise ValueError(\"Only 'constraint' method is supported in this version.\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"Running constraint-based method to get difference undirected graph...\")\n",
    "\n",
    "    n1, n2, p = X1.shape[0], X2.shape[0], X1.shape[1]\n",
    "\n",
    "    # Step 1: Estimate precision matrices\n",
    "    K1 = pinv(np.cov(X1, rowvar=False))\n",
    "    K2 = pinv(np.cov(X2, rowvar=False))\n",
    "    D1 = np.diag(K1)\n",
    "    D2 = np.diag(K2)\n",
    "\n",
    "    # Step 2: Compute test statistic and p-values\n",
    "    stats = (K1 - K2)**2 * (1/((np.outer(D1, D1) + K1**2)/n1 + (np.outer(D2, D2) + K2**2)/n2))\n",
    "    df2 = n1 + n2 - 2*p + 2\n",
    "    pvals = 1 - ncfdtr(1, df2, 0, stats)\n",
    "    pvals = np.clip(pvals, 1e-320, 1.0)\n",
    "\n",
    "    # Step 3: Build Δ-UG\n",
    "    diff_ug = {frozenset({i, j}) for i, j in itr.combinations(range(p), 2) if pvals[i, j] <= alpha}\n",
    "    cond_nodes = {i for edge in diff_ug for i in edge}\n",
    "\n",
    "    # Step 4: Create full p-value list\n",
    "    full_pval_list = [\n",
    "        {\"Node1\": i, \"Node2\": j, \"P_value\": pvals[i, j], \"In_Diff_UG\": pvals[i, j] <= alpha}\n",
    "        for i, j in itr.combinations(range(p), 2)\n",
    "    ]\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"alpha threshold = {alpha}\")\n",
    "        print(f\"Number of Δ-UG edges: {len(diff_ug)}\")\n",
    "        print(\"Difference undirected graph:\", diff_ug)\n",
    "\n",
    "    return diff_ug, cond_nodes, pvals, full_pval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9456f9-9a0d-4011-b3d6-77698746f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from itertools import chain, combinations\n",
    "from scipy.special import ncfdtr\n",
    "\n",
    "# --- MLP Regressor ---\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Train DNN ---\n",
    "def train_model(X, y, epochs=50, lr=0.001):\n",
    "    model = MLPRegressor(X.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Get Conditional SHAP Values; fixed to calculate the values using the conditioning sets, rather than just filtering the values by conditioning set \n",
    "def get_conditional_shap(model, X, i_index, cond_set, nsamples=10000, background_size=50):\n",
    "    feature_set = list(cond_set) + [i_index]\n",
    "    background = X[np.random.choice(X.shape[0], min(background_size, len(X)), replace=False)]\n",
    "    background_subset = background[:, feature_set]\n",
    "\n",
    "    def conditional_predict(x_subset):\n",
    "        x_full = np.tile(X.mean(axis=0), (x_subset.shape[0], 1))\n",
    "        x_full[:, feature_set] = x_subset\n",
    "        x_tensor = torch.tensor(x_full, dtype=torch.float32)\n",
    "        return model(x_tensor).detach().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(conditional_predict, background_subset)\n",
    "    X_test_subset = X[:nsamples, feature_set]\n",
    "    shap_values = explainer.shap_values(X_test_subset)\n",
    "    return np.abs(np.array(shap_values))[:, -1]\n",
    "\n",
    "\n",
    "def shap_ftest(shap1, shap2, df2, normalize=True):\n",
    "    \"\"\"\n",
    "    Perform a normalized SHAP comparison using a squared difference F statistic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap1 : np.ndarray\n",
    "        SHAP values for group 1\n",
    "    shap2 : np.ndarray\n",
    "        SHAP values for group 2\n",
    "    df2 : int\n",
    "        Degrees of freedom (typically: n1 + n2 - 2p + 2)\n",
    "    normalize : bool, default=True\n",
    "        If True, normalize SHAP values across both groups using global mean and std\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stat : float\n",
    "        Test statistic\n",
    "    pval : float\n",
    "        P-value from non-central F distribution\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        all_shap = np.concatenate([shap1, shap2])\n",
    "        mean_all = np.mean(all_shap)\n",
    "        std_all = np.std(all_shap, ddof=1)\n",
    "        std_all = max(std_all, 1e-8)  # prevent divide-by-zero\n",
    "        shap1 = (shap1 - mean_all) / std_all\n",
    "        shap2 = (shap2 - mean_all) / std_all\n",
    "\n",
    "    mu1, mu2 = np.mean(shap1), np.mean(shap2)\n",
    "    s1_sq, s2_sq = np.var(shap1, ddof=1), np.var(shap2, ddof=1)\n",
    "    n1, n2 = len(shap1), len(shap2)\n",
    "\n",
    "    stat = (mu1 - mu2) ** 2 / (s1_sq / n1 + s2_sq / n2)\n",
    "    pval = 1 - ncfdtr(1, df2, 0, stat)\n",
    "\n",
    "    return stat, pval\n",
    "\n",
    "\n",
    "def deepdiff_skeleton_shap_only_with_log(\n",
    "        X1,\n",
    "        X2,\n",
    "        difference_ug: list,\n",
    "        nodes_cond_set: set,\n",
    "        alpha: float = 0.1,\n",
    "        max_set_size: int = 2,\n",
    "        verbose: int = 0,\n",
    "        shap_sample_size: int = 10000\n",
    "):\n",
    "    skeleton = set(difference_ug)\n",
    "    pval_log_by_size = {r: [] for r in range(max_set_size + 1)}\n",
    "    shap_store_by_size = {r: {} for r in range(max_set_size + 1)}\n",
    "\n",
    "    for r in range(max_set_size + 1):  # Conditioning set sizes 0, 1, 2\n",
    "        for i, j in list(skeleton):  # Only test currently surviving edges\n",
    "            for cond_set in combinations(nodes_cond_set - {i, j}, r):\n",
    "                cond_list = list(cond_set)\n",
    "\n",
    "                # i ~ j + S\n",
    "                model1_i = train_model(X1, X1[:, i])\n",
    "                model2_i = train_model(X2, X2[:, i])\n",
    "\n",
    "                shap1_i = get_conditional_shap(model1_i, X1, i_index=j, cond_set=cond_set, nsamples=shap_sample_size)\n",
    "                shap2_i = get_conditional_shap(model2_i, X2, i_index=j, cond_set=cond_set, nsamples=shap_sample_size)\n",
    "\n",
    "                shap_store_by_size[r][(i, j, 'i<-j', 'X1')] = shap1_i\n",
    "                shap_store_by_size[r][(i, j, 'i<-j', 'X2')] = shap2_i\n",
    "\n",
    "                df2_i = len(shap1_i) + len(shap2_i) - 2 - 2 * len(cond_list)\n",
    "                stat_i, pval_i = shap_ftest(shap1_i, shap2_i, df2=df2_i)\n",
    "\n",
    "                row_i = {\n",
    "                    \"From\": j,\n",
    "                    \"To\": i,\n",
    "                    \"Conditioning_Set\": tuple(cond_list),\n",
    "                    \"Direction\": f\"{i} <- {j}\",\n",
    "                    \"Statistic\": stat_i,\n",
    "                    \"P_value\": pval_i,\n",
    "                    \"Removed\": pval_i > alpha\n",
    "                }\n",
    "                pval_log_by_size[r].append(row_i)\n",
    "\n",
    "                if pval_i > alpha:\n",
    "                    if verbose == 1:\n",
    "                        print(\n",
    "                            f\"(r={r}) Removing edge {j}->{i} since p-value={pval_i:.5f} > alpha={alpha:.5f} with cond set {cond_list}\"\n",
    "                        )\n",
    "                    skeleton.discard((i, j))\n",
    "                    break\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        print(\n",
    "                            f\"(r={r}) {j}->{i} kept: p-value={pval_i:.5f} < alpha={alpha:.5f} with cond set {cond_list}\"\n",
    "                        )\n",
    "\n",
    "                # j ~ i + S\n",
    "                model1_j = train_model(X1, X1[:, j])\n",
    "                model2_j = train_model(X2, X2[:, j])\n",
    "\n",
    "                shap1_j = get_conditional_shap(model1_j, X1, i_index=i, cond_set=cond_set, nsamples=shap_sample_size)\n",
    "                shap2_j = get_conditional_shap(model2_j, X2, i_index=i, cond_set=cond_set, nsamples=shap_sample_size)\n",
    "\n",
    "                shap_store_by_size[r][(j, i, 'j<-i', 'X1')] = shap1_j\n",
    "                shap_store_by_size[r][(j, i, 'j<-i', 'X2')] = shap2_j\n",
    "\n",
    "                df2_j = len(shap1_j) + len(shap2_j) - 2 - 2 * len(cond_list)\n",
    "                stat_j, pval_j = shap_ftest(shap1_j, shap2_j, df2=df2_j)\n",
    "\n",
    "                row_j = {\n",
    "                    \"From\": i,\n",
    "                    \"To\": j,\n",
    "                    \"Conditioning_Set\": tuple(cond_list),\n",
    "                    \"Direction\": f\"{j} <- {i}\",\n",
    "                    \"Statistic\": stat_j,\n",
    "                    \"P_value\": pval_j,\n",
    "                    \"Removed\": pval_j > alpha\n",
    "                }\n",
    "                pval_log_by_size[r].append(row_j)\n",
    "\n",
    "                if pval_j > alpha:\n",
    "                    if verbose == 1:\n",
    "                        print(\n",
    "                            f\"(r={r}) Removing edge {i}->{j} since p-value={pval_j:.5f} > alpha={alpha:.5f} with cond set {cond_list}\"\n",
    "                        )\n",
    "                    skeleton.discard((i, j))\n",
    "                    break\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        print(\n",
    "                            f\"(r={r}) {i}->{j} kept: p-value={pval_j:.5f} < alpha={alpha:.5f} with cond set {cond_list}\"\n",
    "                        )\n",
    "\n",
    "    return skeleton, {r: pd.DataFrame(pval_log_by_size[r]) for r in pval_log_by_size}, shap_store_by_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b79cb-fdf9-408b-b5a6-cae89aa2b599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from scipy.special import ncfdtr\n",
    "\n",
    "# Simple MLP for regression\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Estimate residual variance using DNN\n",
    "def dnn_residual_variance(X, y, epochs=50, lr=0.001):\n",
    "    model = MLPRegressor(X.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        residuals = y_tensor - model(X_tensor)\n",
    "        return np.var(residuals.numpy().flatten())\n",
    "\n",
    "# Convert edges to adjacency matrix\n",
    "def edges2adjacency(num_nodes, edge_set, undirected=False):\n",
    "    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    for parent, child in edge_set:\n",
    "        adjacency_matrix[parent, child] = 1\n",
    "        if undirected:\n",
    "            adjacency_matrix[child, parent] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Main function: orientation\n",
    "def deepdiff_orient_dnn(\n",
    "        X1, \n",
    "        X2,\n",
    "        skeleton: set,\n",
    "        nodes_cond_set: set,  # kept for interface compatibility\n",
    "        alpha: float = 0.1,\n",
    "        verbose: int = 0\n",
    "):\n",
    "    n1, n2 = X1.shape[0], X2.shape[0]\n",
    "    p = X1.shape[1]\n",
    "\n",
    "    skeleton_nodes = {i for edge in skeleton for i in edge}\n",
    "    skeleton_frozen = {frozenset(edge) for edge in skeleton}\n",
    "\n",
    "    oriented_edges = set()\n",
    "    orientation_log = {}\n",
    "    nodes_with_decided_parents = set()\n",
    "    d_nx = nx.DiGraph()\n",
    "    d_nx.add_nodes_from(skeleton_nodes)\n",
    "\n",
    "    # Conditioning set size = 1 only\n",
    "    k = 1\n",
    "    if verbose > 0:\n",
    "        print(f\"--- Conditioning sets of size {k} ---\")\n",
    "\n",
    "    for j in skeleton_nodes - nodes_with_decided_parents:\n",
    "        candidates = skeleton_nodes - {j}\n",
    "        for S in combinations(candidates, k):\n",
    "            if frozenset({j, S[0]}) not in skeleton_frozen:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                var1 = dnn_residual_variance(X1[:, S], X1[:, j])\n",
    "                var2 = dnn_residual_variance(X2[:, S], X2[:, j])\n",
    "                pval = ncfdtr(n1 - k, n2 - k, 0, var1 / var2)\n",
    "                pval = 2 * min(pval, 1 - pval)\n",
    "            except:\n",
    "                pval = 0.0\n",
    "\n",
    "            orientation_log[(j, S)] = {\n",
    "                \"Node\": j,\n",
    "                \"Conditioning_Set\": S,\n",
    "                \"P_value\": pval,\n",
    "                \"Directions\": [f\"{p} → {j}\" for p in S],\n",
    "                \"Accepted\": False\n",
    "            }\n",
    "\n",
    "            if pval > alpha:\n",
    "                S_set = set(S)\n",
    "                rest = skeleton_nodes - S_set - {j}\n",
    "\n",
    "                parent_edges = {(p, j) for p in S if frozenset({p, j}) in skeleton_frozen}\n",
    "                child_edges = {(j, c) for c in rest if frozenset({j, c}) in skeleton_frozen}\n",
    "                candidate_edges = parent_edges | child_edges\n",
    "\n",
    "                # Cycle/contradiction checks\n",
    "                if any(p in d_nx.successors(j) for p in S):\n",
    "                    continue\n",
    "                if any(c in d_nx.predecessors(j) for c in rest):\n",
    "                    continue\n",
    "                if any(p in nx.descendants(d_nx, j) for p in S):\n",
    "                    continue\n",
    "                if any(c in nx.ancestors(d_nx, j) for c in rest):\n",
    "                    continue\n",
    "\n",
    "                oriented_edges.update(candidate_edges)\n",
    "                d_nx.add_edges_from(candidate_edges)\n",
    "                nodes_with_decided_parents.add(j)\n",
    "                orientation_log[(j, S)][\"Accepted\"] = True\n",
    "\n",
    "                if verbose > 0:\n",
    "                    print(f\"Adding {candidate_edges}\")\n",
    "                break  # move to next j\n",
    "\n",
    "    # graph traversal to orient remaining edges if possible\n",
    "    unoriented_edges_before = skeleton_frozen - {frozenset((i, j)) for i, j in oriented_edges}\n",
    "    unoriented_edges = unoriented_edges_before.copy()\n",
    "\n",
    "    for edge in unoriented_edges_before:\n",
    "        i, j = tuple(edge)\n",
    "        if list(nx.all_simple_paths(d_nx, source=i, target=j)):\n",
    "            oriented_edges.add((i, j))\n",
    "            unoriented_edges.remove(frozenset((i, j)))\n",
    "            if verbose > 0:\n",
    "                print(f\"Oriented ({i}, {j}) as ({i}, {j}) with graph traversal\")\n",
    "        elif list(nx.all_simple_paths(d_nx, source=j, target=i)):\n",
    "            oriented_edges.add((j, i))\n",
    "            unoriented_edges.remove(frozenset((i, j)))\n",
    "            if verbose > 0:\n",
    "                print(f\"Oriented ({i}, {j}) as ({j}, {i}) with graph traversal\")\n",
    "\n",
    "    adjacency_matrix = edges2adjacency(p, unoriented_edges, undirected=True) + \\\n",
    "                       edges2adjacency(p, oriented_edges, undirected=False)\n",
    "\n",
    "    orient_log = sorted(list(orientation_log.values()), key=lambda d: (len(d['Conditioning_Set']), d['Node']))\n",
    "    return adjacency_matrix, orient_log, unoriented_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db884fdb-6adc-488b-8aea-1ecb6d5ffb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_full_deepdiff_all_levels(\n",
    "    X1,\n",
    "    X2,\n",
    "    alpha_ug=0.00000005,\n",
    "    alpha_skel=0.1,\n",
    "    alpha_orient=0.05,\n",
    "    max_set_size=1,\n",
    "    verbose=1,\n",
    "    shap_sample_size=1000\n",
    "):\n",
    "    if verbose:\n",
    "        print(\"Step 1: Estimating Δ-UG (difference undirected graph)...\")\n",
    "\n",
    "    diff_ug_raw, cond_nodes, pvals, full_ug_table = deepdiff_shap_undirected_graph(X1, X2, alpha=alpha_ug)\n",
    "    diff_ug = [tuple(sorted(list(edge))) for edge in diff_ug_raw]\n",
    "    if verbose:\n",
    "        print(\"Undirected difference DAG:\", diff_ug)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Step 2: Pruning to skeletons via SHAP invariance...\")\n",
    "\n",
    "    skeleton, shap_df_by_size, shap_store_by_size = deepdiff_skeleton_shap_only_with_log(\n",
    "        X1, X2,\n",
    "        difference_ug=diff_ug,\n",
    "        nodes_cond_set=cond_nodes,\n",
    "        alpha=alpha_skel,\n",
    "        max_set_size=max_set_size,\n",
    "        verbose=verbose,\n",
    "        shap_sample_size=shap_sample_size\n",
    "    )\n",
    "\n",
    "    results_by_r = {}\n",
    "\n",
    "    # Just do orientation at r = 1\n",
    "    r = 1\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Step 3: Orientation for conditioning set size r = {r} ---\")\n",
    "        print(\"Skeleton edges:\", skeleton)\n",
    "\n",
    "    adj_matrix, orient_log, unoriented_edges = deepdiff_orient_dnn(\n",
    "        X1, X2,\n",
    "        skeleton=skeleton,\n",
    "        nodes_cond_set=cond_nodes,\n",
    "        alpha=alpha_orient,\n",
    "        verbose=verbose  # max_set_size no longer needed\n",
    "    )\n",
    "\n",
    "    results_by_r[r] = {\n",
    "        \"skeleton\": skeleton,\n",
    "        \"adj_matrix\": adj_matrix,\n",
    "        \"orient_log\": orient_log,\n",
    "        \"unoriented_edges\": unoriented_edges,\n",
    "        \"shap_df\": shap_df_by_size[r],\n",
    "        \"shap_store\": shap_store_by_size[r],\n",
    "        \"diff_ug\": diff_ug,\n",
    "        \"pvals\": pvals, \n",
    "        \"diff_ug_full_table\": pd.DataFrame(full_ug_table)\n",
    "    }\n",
    "\n",
    "    return diff_ug, results_by_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429ca7b-ccac-41de-88cb-f5697e423826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_ug, all_results = run_full_deepdiff_all_levels(\n",
    "    X1, X2,\n",
    "    alpha_ug=0.005,\n",
    "    alpha_skel=0.3,\n",
    "    alpha_orient=0.001,\n",
    "    max_set_size=1,   # This still controls SHAP skeleton pruning\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# # Only orientation for r = 1 is available\n",
    "# r1 = all_results[1]\n",
    "\n",
    "# # Access components\n",
    "# adj_r1 = r1[\"adj_matrix\"]\n",
    "# log_r1 = r1[\"orient_log\"]\n",
    "# skeleton_r1 = r1[\"skeleton\"]\n",
    "# shap_df_r1 = r1[\"shap_df\"]\n",
    "# shap_store_r1 = r1[\"shap_store\"]\n",
    "# unoriented_edges_r1 = r1[\"unoriented_edges\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f6bda-7162-4e0c-8a04-44226bac4092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# r1[\"shap_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf19486-89a7-46dd-bcc8-3764d0330f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_orient_log(log):\n",
    "    return [\n",
    "        {\n",
    "            \"Node\": e[\"Node\"],\n",
    "            \"Conditioning_Set\": \",\".join(map(str, sorted(e[\"Conditioning_Set\"]))),\n",
    "            \"P_value\": e[\"P_value\"],\n",
    "            \"Directions\": \",\".join(e[\"Directions\"]),\n",
    "            \"Accepted\": e[\"Accepted\"]\n",
    "        }\n",
    "        for e in log\n",
    "    ]\n",
    "\n",
    "def save_deepdiff_shap_result(result_dict, prefix):\n",
    "    # 1. Save orient_log\n",
    "    if \"orient_log\" in result_dict:\n",
    "        orient_log = flatten_orient_log(result_dict[\"orient_log\"])\n",
    "        df_log = pd.DataFrame(orient_log)\n",
    "        df_log.to_csv(f\"{prefix}_orient_log.csv\", index=False)\n",
    "\n",
    "    # 2. Save skeleton\n",
    "    if \"skeleton\" in result_dict:\n",
    "        skeleton = list(result_dict[\"skeleton\"])\n",
    "        df_skel = pd.DataFrame(skeleton, columns=[\"Node1\", \"Node2\"])\n",
    "        df_skel.to_csv(f\"{prefix}_skeleton.csv\", index=False)\n",
    "\n",
    "    # 3. Save adjacency matrix\n",
    "    if \"adj_matrix\" in result_dict:\n",
    "        adj = result_dict[\"adj_matrix\"]\n",
    "        np.savetxt(f\"{prefix}_adj_matrix.csv\", adj, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "    # 4. Save SHAP skeleton pruning log\n",
    "    if \"shap_df\" in result_dict and isinstance(result_dict[\"shap_df\"], pd.DataFrame):\n",
    "        df_skeleton_log = result_dict[\"shap_df\"].copy()\n",
    "        df_skeleton_log[\"Conditioning_Set\"] = df_skeleton_log[\"Conditioning_Set\"].apply(\n",
    "            lambda x: \",\".join(map(str, x)) if isinstance(x, (list, tuple)) else str(x)\n",
    "        )\n",
    "        df_skeleton_log.to_csv(f\"{prefix}_skeleton_log.csv\", index=False)\n",
    "\n",
    "    # 5. Save full p-value matrix and Δ-UG edge summary\n",
    "    if \"pvals\" in result_dict and isinstance(result_dict[\"pvals\"], np.ndarray):\n",
    "        pvals = result_dict[\"pvals\"]\n",
    "        n = pvals.shape[0]\n",
    "        if \"diff_ug\" in result_dict:\n",
    "            ug_set = set(map(tuple, map(sorted, result_dict[\"diff_ug\"])))\n",
    "        else:\n",
    "            ug_set = set()\n",
    "\n",
    "        rows = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                rows.append({\n",
    "                    \"Node1\": i,\n",
    "                    \"Node2\": j,\n",
    "                    \"P_value\": pvals[i, j],\n",
    "                    \"In_Diff_UG\": (i, j) in ug_set\n",
    "                })\n",
    "        df_pvals = pd.DataFrame(rows)\n",
    "        df_pvals.to_csv(f\"{prefix}_undirected_graph_pvals.csv\", index=False)\n",
    "        np.savetxt(f\"{prefix}_pval_matrix.csv\", pvals, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503306cb-cf13-45e8-a8ab-451487e74bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_deepdiff_shap_result(all_results[1], prefix=\"r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cde910-a69c-445b-9ec2-736c1e536989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_oriented_graph(adj_matrix, r, node_labels=None, layout='spring', spacing=2.0, figsize=(12, 10)):\n",
    "#     G = nx.from_numpy_array(adj_matrix, create_using=nx.DiGraph)\n",
    "\n",
    "#     if layout == 'spring':\n",
    "#         pos = nx.spring_layout(G, seed=42, k=spacing, scale=spacing*1.5)\n",
    "#     elif layout == 'circular':\n",
    "#         pos = nx.circular_layout(G)\n",
    "#     elif layout == 'kamada_kawai':\n",
    "#         pos = nx.kamada_kawai_layout(G)\n",
    "#     else:\n",
    "#         pos = nx.spring_layout(G, seed=42, k=spacing)\n",
    "\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     nx.draw_networkx_nodes(G, pos, node_size=500, node_color=\"skyblue\")\n",
    "#     nx.draw_networkx_edges(G, pos, arrowstyle='-|>', arrowsize=15, edge_color='gray')\n",
    "\n",
    "#     if node_labels:\n",
    "#         nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10)\n",
    "#     else:\n",
    "#         nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "#     plt.title(f\"Oriented Graph (Conditioning Set Size r = {r})\")\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66e6aa-e439-47ad-8858-5dd2f21eaa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for r, result in all_results.items():\n",
    "#     print(f\"Showing graph for conditioning set size r = {r}\")\n",
    "#     plot_oriented_graph(result[\"adj_matrix\"], r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044dac2-9f0a-40cd-81aa-5c7bd42936ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
